# =========================================
# LOGSTASH PIPELINE - FastFood Delivery
# =========================================

input {
  # TCP input for application logs
  tcp {
    port => 5000
    codec => json_lines
    tags => ["application"]
  }

  # UDP input for syslog
  udp {
    port => 5000
    codec => json_lines
    tags => ["application"]
  }

  # Beats input (Filebeat)
  beats {
    port => 5044
    tags => ["beats"]
  }

  # HTTP input for direct log shipping
  http {
    port => 8080
    codec => json
    tags => ["http"]
  }
}

filter {
  # Parse JSON logs
  if [message] =~ /^\{/ {
    json {
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
    }
  }

  # Add service name from container name
  if [container][name] {
    mutate {
      add_field => { "service" => "%{[container][name]}" }
    }
  }

  # Parse timestamp
  if [parsed][timestamp] {
    date {
      match => ["[parsed][timestamp]", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
      target => "@timestamp"
    }
  }

  # Extract log level
  if [parsed][level] {
    mutate {
      add_field => { "log_level" => "%{[parsed][level]}" }
    }
  } else if [message] =~ /ERROR|error/ {
    mutate {
      add_field => { "log_level" => "error" }
    }
  } else if [message] =~ /WARN|warn/ {
    mutate {
      add_field => { "log_level" => "warn" }
    }
  } else if [message] =~ /INFO|info/ {
    mutate {
      add_field => { "log_level" => "info" }
    }
  } else if [message] =~ /DEBUG|debug/ {
    mutate {
      add_field => { "log_level" => "debug" }
    }
  } else {
    mutate {
      add_field => { "log_level" => "info" }
    }
  }

  # Extract HTTP request details
  if [parsed][method] and [parsed][url] {
    mutate {
      add_field => {
        "http_method" => "%{[parsed][method]}"
        "http_url" => "%{[parsed][url]}"
      }
    }
  }

  # Extract response time
  if [parsed][responseTime] {
    mutate {
      add_field => { "response_time_ms" => "%{[parsed][responseTime]}" }
    }
    mutate {
      convert => { "response_time_ms" => "integer" }
    }
  }

  # Extract user/request ID
  if [parsed][requestId] {
    mutate {
      add_field => { "request_id" => "%{[parsed][requestId]}" }
    }
  }

  if [parsed][userId] {
    mutate {
      add_field => { "user_id" => "%{[parsed][userId]}" }
    }
  }

  # Tag by service type
  if [service] =~ /auth/ {
    mutate {
      add_tag => ["auth-service"]
    }
  } else if [service] =~ /order/ {
    mutate {
      add_tag => ["order-service"]
    }
  } else if [service] =~ /restaurant/ {
    mutate {
      add_tag => ["restaurant-service"]
    }
  } else if [service] =~ /payment/ {
    mutate {
      add_tag => ["payment-service"]
    }
  } else if [service] =~ /notification/ {
    mutate {
      add_tag => ["notification-service"]
    }
  } else if [service] =~ /delivery|food-delivery/ {
    mutate {
      add_tag => ["delivery-service"]
    }
  } else if [service] =~ /admin/ {
    mutate {
      add_tag => ["admin-service"]
    }
  }

  # Tag errors for alerting
  if [log_level] == "error" {
    mutate {
      add_tag => ["error", "alert"]
    }
  }

  # Geoip for IP addresses
  if [parsed][clientIp] {
    geoip {
      source => "[parsed][clientIp]"
      target => "geoip"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "agent", "ecs"]
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "fastfood-logs-%{+YYYY.MM.dd}"
    template_name => "fastfood-logs"
    template_overwrite => true
  }

  # Debug output (comment in production)
  # stdout {
  #   codec => rubydebug
  # }

  # Send errors to separate index
  if "error" in [tags] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "fastfood-errors-%{+YYYY.MM.dd}"
    }
  }
}
